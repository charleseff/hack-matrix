{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HackMatrix PureJaxRL Training on TPU (Colab)\n\nThis notebook trains the HackMatrix game using PureJaxRL on Google Colab's free TPUs.\n\n**Before running:**\n1. Verify TPU v5e-1 is selected (Runtime > Change runtime type) - should be auto-selected\n2. Run cells in order\n3. The training script will automatically verify TPU is detected"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/charleseff/hack-matrix.git\n",
    "%cd hack-matrix/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and enable JAX compilation cache\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Enable compilation caching (speeds up subsequent runs)\n",
    "import os\n",
    "\n",
    "os.environ[\"JAX_COMPILATION_CACHE_DIR\"] = \"/content/jax_cache\"\n",
    "os.makedirs(\"/content/jax_cache\", exist_ok=True)\n",
    "\n",
    "print(\"\u2705 Dependencies installed\")\n",
    "print(\"\u2705 JAX compilation cache enabled at /content/jax_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Quick Test (~100K timesteps)\n\nThis will verify TPU is working and show device information. Takes ~30 seconds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test - uses smaller batch to verify everything works\n",
    "!python scripts/train_purejaxrl.py \\\n",
    "  --num-envs 64 \\\n",
    "  --num-steps 64 \\\n",
    "  --total-timesteps 100000 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Medium Training (500K timesteps)\n\nLarger batch for better TPU utilization. Compiles faster on 2nd run due to cache."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium training - moderate batch size (256 * 128 = 32K batch)\n",
    "!python scripts/train_purejaxrl.py \\\n",
    "  --num-envs 256 \\\n",
    "  --num-steps 128 \\\n",
    "  --total-timesteps 500000 \\\n",
    "  --save-interval 10 \\\n",
    "  --log-interval 5 \\\n",
    "  --checkpoint-dir checkpoints/colab_medium \\\n",
    "  --seed 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Training (10M timesteps)\n",
    "\n",
    "This should take 5-10 minutes on TPU. Adjust parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training - large batch for TPU (1024 * 256 = 262K batch)\n",
    "!python scripts/train_purejaxrl.py \\\n",
    "  --num-envs 1024 \\\n",
    "  --num-steps 256 \\\n",
    "  --total-timesteps 10000000 \\\n",
    "  --lr 0.0003 \\\n",
    "  --num-minibatches 8 \\\n",
    "  --update-epochs 4 \\\n",
    "  --hidden-dim 512 \\\n",
    "  --num-layers 3 \\\n",
    "  --save-interval 100 \\\n",
    "  --log-interval 10 \\\n",
    "  --checkpoint-dir checkpoints/colab_full \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Checkpoints\n",
    "\n",
    "Download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available checkpoints\n",
    "!ls -lh checkpoints/\n",
    "\n",
    "# Download final checkpoint\n",
    "from google.colab import files\n",
    "\n",
    "files.download(\"checkpoints/final_params.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "### TPU Performance\n",
    "- Colab TPUs are v2-8 (8 cores) or v3-8 depending on availability\n",
    "- Expected throughput: 50K-100K steps/second\n",
    "- 10M timesteps should take 5-10 minutes\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- `--num-envs`: More envs = better TPU utilization (try 1024-4096)\n",
    "- `--num-steps`: Longer rollouts = more stable gradients (try 256-1024)\n",
    "- `--learning-rate`: Start with 0.0003, reduce if training unstable\n",
    "- `--hidden-dim`: Larger network = more capacity (try 256-1024)\n",
    "\n",
    "### Memory Issues\n",
    "If you run out of memory:\n",
    "- Reduce `--num-envs` (try 1024 instead of 2048)\n",
    "- Reduce `--hidden-dim` (try 256 instead of 512)\n",
    "- Reduce `--num-steps` (try 256 instead of 512)\n",
    "\n",
    "### Session Limits\n",
    "- Colab has a 12-hour session limit (free tier)\n",
    "- Download checkpoints regularly to avoid losing progress\n",
    "- For longer training, consider TRC program or Colab Pro"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}