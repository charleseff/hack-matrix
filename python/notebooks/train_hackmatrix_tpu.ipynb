{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HackMatrix PureJaxRL Training on TPU (Colab)\n",
    "\n",
    "This notebook trains the HackMatrix game using PureJaxRL on Google Colab's free TPUs.\n",
    "\n",
    "**Before running:**\n",
    "1. Runtime → Change runtime type → **TPU** (important!)\n",
    "2. Run cells in order\n",
    "3. The training script will automatically verify TPU is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/charleseff/hack-matrix.git\n",
    "%cd hack-matrix/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies and enable JAX compilation cache\n!pip install -q -r requirements.txt\n\n# Enable compilation caching (speeds up subsequent runs)\nimport os\nos.environ[\"JAX_COMPILATION_CACHE_DIR\"] = \"/content/jax_cache\"\nos.makedirs(\"/content/jax_cache\", exist_ok=True)\n\nprint(\"✅ Dependencies installed\")\nprint(\"✅ JAX compilation cache enabled at /content/jax_cache\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Quick Test (~100K timesteps)\n\nThis will verify TPU is working and show device information. Takes ~30 seconds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick test - uses smaller batch to verify everything works\n!python scripts/train_purejaxrl.py \\\n  --num-envs 64 \\\n  --num-steps 64 \\\n  --total-timesteps 100000 \\\n  --seed 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Medium Training (500K timesteps)\n\nLarger batch for better TPU utilization. Compiles faster on 2nd run due to cache."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Medium training - moderate batch size (256 * 128 = 32K batch)\n!python scripts/train_purejaxrl.py \\\n  --num-envs 256 \\\n  --num-steps 128 \\\n  --total-timesteps 500000 \\\n  --save-interval 10 \\\n  --log-interval 5 \\\n  --checkpoint-dir checkpoints/colab_medium \\\n  --seed 123"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Training (10M timesteps)\n",
    "\n",
    "This should take 5-10 minutes on TPU. Adjust parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Full training - large batch for TPU (1024 * 256 = 262K batch)\n!python scripts/train_purejaxrl.py \\\n  --num-envs 1024 \\\n  --num-steps 256 \\\n  --total-timesteps 10000000 \\\n  --lr 0.0003 \\\n  --num-minibatches 8 \\\n  --update-epochs 4 \\\n  --hidden-dim 512 \\\n  --num-layers 3 \\\n  --save-interval 100 \\\n  --log-interval 10 \\\n  --checkpoint-dir checkpoints/colab_full \\\n  --seed 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Checkpoints\n",
    "\n",
    "Download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available checkpoints\n",
    "!ls -lh checkpoints/\n",
    "\n",
    "# Download final checkpoint\n",
    "from google.colab import files\n",
    "files.download('checkpoints/final_params.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "### TPU Performance\n",
    "- Colab TPUs are v2-8 (8 cores) or v3-8 depending on availability\n",
    "- Expected throughput: 50K-100K steps/second\n",
    "- 10M timesteps should take 5-10 minutes\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- `--num-envs`: More envs = better TPU utilization (try 1024-4096)\n",
    "- `--num-steps`: Longer rollouts = more stable gradients (try 256-1024)\n",
    "- `--learning-rate`: Start with 0.0003, reduce if training unstable\n",
    "- `--hidden-dim`: Larger network = more capacity (try 256-1024)\n",
    "\n",
    "### Memory Issues\n",
    "If you run out of memory:\n",
    "- Reduce `--num-envs` (try 1024 instead of 2048)\n",
    "- Reduce `--hidden-dim` (try 256 instead of 512)\n",
    "- Reduce `--num-steps` (try 256 instead of 512)\n",
    "\n",
    "### Session Limits\n",
    "- Colab has a 12-hour session limit (free tier)\n",
    "- Download checkpoints regularly to avoid losing progress\n",
    "- For longer training, consider TRC program or Colab Pro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}